


import pandas as pd
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np





wine_data = load_wine(as_frame=True)
wine_df = wine_data.frame

wine_df.head().T


wine_df.hist(figsize=(10, 8))
plt.show()


numeric_cols = wine_df.select_dtypes(include=np.number).columns.tolist()

# 3. Initialize and fit the StandardScaler
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])





# Display the shape of the DataFrame
print("Shape of the Wine DataFrame:", wine_df.shape)





# create train and test dataset by splitting original dataset
X = wine_df.drop(columns=['target'], axis=1)
y = wine_df['target']

X_train, X_test, \
    y_train, y_test = train_test_split(X,y ,
                                   random_state=104, 
                                   test_size=0.3, 
                                   stratify=y,
                                   shuffle=True)

print(f"X_train.shape: {X_train.shape}")
print(f"y_train.shape: {y_train.shape}")
print("*" * 10)
print(f"X_test.shape: {X_test.shape}")
print(f"y_test.shape: {y_test.shape}")


categories = np.unique(y)
# Positions of the bars
bins = np.arange(len(categories))
width = 0.35   # width of each bar

plt.figure(figsize=(8, 5))

plt.hist(y_train, bins=len(categories), alpha=0.6, label='train')

plt.hist(y_test, bins=len(categories), alpha=0.6, label='test')

plt.xlabel("Value")
plt.ylabel("Frequency")
plt.title("labels frequencies")
plt.legend()
plt.show()





# train knn with k=3
clf = KNeighborsClassifier(n_neighbors = 3)
clf.fit(X_train, y_train)

# calculate score on train dataset
training_score = clf.score(X_train, y_train)
print(f"training_score: {training_score}")


# calculate test score
test_score = clf.score(X_test, y_test)
print(f"test_score: {test_score}")





def plot_conf_matrices(X_train, X_test, model):
    # Get predictions for the training set
    y_train_pred = model.predict(X_train)
    # Get predictions for the test set
    y_test_pred = model.predict(X_test)

    # --- 3. Calculate Confusion Matrices ---
    cm_train = confusion_matrix(y_train, y_train_pred)
    cm_test = confusion_matrix(y_test, y_test_pred)

    # Define labels for the axes
    class_labels = [f'Class {i}' for i in model.classes_]

    # --- 4. Plot Side-by-Side ---
    # Create a figure with 1 row and 2 columns (for the two matrices)
    fig, axes = plt.subplots(1, 2, figsize=(16, 6)) # Adjust figsize as needed

    # --- Plot 1: Training Confusion Matrix ---
    sns.heatmap(
        cm_train, 
        annot=True,              # Annotate cells with counts
        fmt='d',                 # Format annotations as integers
        cmap='Blues',            # Color map
        cbar=False,              # No separate color bar needed
        linewidths=0.5,          # Lines between cells
        xticklabels=class_labels,
        yticklabels=class_labels,
        ax=axes[0]               # Plot on the first subplot
    )
    axes[0].set_title('Training Set Confusion Matrix', fontsize=16)
    axes[0].set_ylabel('True Class', fontsize=12)
    axes[0].set_xlabel('Predicted Class', fontsize=12)
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].tick_params(axis='y', rotation=0)


    # --- Plot 2: Test Confusion Matrix ---
    sns.heatmap(
        cm_test, 
        annot=True, 
        fmt='d', 
        cmap='Blues', 
        cbar=False, 
        linewidths=0.5, 
        xticklabels=class_labels,
        yticklabels=class_labels,
        ax=axes[1]               # Plot on the second subplot
    )
    axes[1].set_title('Test Set Confusion Matrix', fontsize=16)
    axes[1].set_ylabel('True Class', fontsize=12)
    axes[1].set_xlabel('Predicted Class', fontsize=12)
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].tick_params(axis='y', rotation=0)


    plt.suptitle('Comparison of Training vs. Test Set Performance', fontsize=18, y=1.02)
    plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent title overlap
    plt.show()

plot_conf_matrices(X_train, X_test, clf)





# make data standard
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# apply pca
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"X_train_pca.shape: {X_train_pca.shape}")
print(f"X_test_pca.shape: {X_test_pca.shape}")


# train knn model using transformed dataset
# train knn with k=3
clf = KNeighborsClassifier(n_neighbors = 3)
clf.fit(X_train_pca, y_train)

# calculate score on train dataset
training_score = clf.score(X_train_pca, y_train)
print(f"training_score: {training_score}")


# calculate test score
test_score = clf.score(X_test_pca, y_test)
print(f"test_score: {test_score}")


plot_conf_matrices(X_train_pca, X_test_pca, clf)








def predict(X):
    if X['alcohol'] < 12:
        return 0
    elif X['alcohol'] >= 12 and X['alcohol'] < 13:
        return 1
    else:
        return 2


y_train_pred = X_train.apply(predict, axis=1)
y_test_pred = X_test.apply(predict, axis=1)


print(f"train_score: {np.mean(y_train == y_train_pred)}")
print(f"test_score: {np.mean(y_test == y_test_pred)}")





models = {}
for metric in ['cosine', 'manhattan', 'euclidean', 'chebyshev', 'mahalanobis']:
    if metric == 'mahalanobis':
        cov_matrix = np.cov(X_train, rowvar=False) 
        inv_cov_matrix = np.linalg.inv(cov_matrix)
        clf = KNeighborsClassifier(
            n_neighbors=3,
            metric='mahalanobis',
            metric_params={'V': inv_cov_matrix})
    else:
        clf = KNeighborsClassifier(n_neighbors = 3, metric=metric)
    clf.fit(X_train, y_train)
    print(f"{metric} with score: {clf.score(X_test, y_test)}")








regr = LinearRegression()

regr.fit(X_train, y_train)


def score(clf, X, y, threshold):
    y_pred = clf.predict(X)
    y_pred = np.where(y_pred < threshold, 0, y_pred)
    y_pred = np.where((y_pred >= threshold) & (y_pred < threshold * 2), 1, y_pred)
    y_pred = np.where(y_pred >= threshold * 2, 2, y_pred)
    return y_pred, np.mean(y_pred == y)





train_scores = []
test_scores = []
thresholds = np.linspace(0, 2, 20)
for threshold in thresholds:
    print(f"calculating score for threshold: {threshold}")
    train_scores.append(score(regr, X_train, y_train, threshold)[1])
    test_scores.append(score(regr, X_test, y_test, threshold)[1])


plt.plot(thresholds, train_scores, 'bo-', label='train score')
plt.plot(thresholds, test_scores, 'ro-', label='test score')
plt.annotate(
    'best threshold',                
    xy=(0.75, .94),             
    xytext=(0.5, 1.2),                 
    arrowprops=dict(facecolor='black', shrink=0.05), 
    fontsize=12,
    color='green'
)
plt.title("regression scores on train/test datasets")
plt.ylabel("score")
plt.xlabel("threshold")
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()


def plot_conf_matrix_2(threshold):
    y_train_pred = score(regr, X_train, y_train, threshold)[0]

    y_test_pred = score(regr, X_test, y_test, threshold)[0]


    cm_train = confusion_matrix(y_train, y_train_pred)
    cm_test = confusion_matrix(y_test, y_test_pred)

    class_labels = [f'Class {i}' for i in [0, 1, 2]]

    fig, axes = plt.subplots(1, 2, figsize=(16, 6)) 

    sns.heatmap(
        cm_train, 
        annot=True,              
        fmt='d',                 
        cmap='Blues',            
        cbar=False,              
        linewidths=0.5,          
        xticklabels=class_labels,
        yticklabels=class_labels,
        ax=axes[0]              
    )
    axes[0].set_title('Training Set Confusion Matrix', fontsize=16)
    axes[0].set_ylabel('True Class', fontsize=12)
    axes[0].set_xlabel('Predicted Class', fontsize=12)
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].tick_params(axis='y', rotation=0)


    sns.heatmap(
        cm_test, 
        annot=True, 
        fmt='d', 
        cmap='Blues', 
        cbar=False, 
        linewidths=0.5, 
        xticklabels=class_labels,
        yticklabels=class_labels,
        ax=axes[1]               
    )
    axes[1].set_title('Test Set Confusion Matrix', fontsize=16)
    axes[1].set_ylabel('True Class', fontsize=12)
    axes[1].set_xlabel('Predicted Class', fontsize=12)
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].tick_params(axis='y', rotation=0)


    plt.suptitle(f'Comparison of Training vs. Test Set Performance for threshold: {threshold}', fontsize=18, y=1.02)
    plt.tight_layout(rect=[0, 0, 1, 0.98]) 
    plt.show()


for threshold in thresholds:
    plot_conf_matrix_2(threshold)


x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1
y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1

print(f"x_min: {x_min}, \t x_max: {x_max}")
print(f"y_min: {y_min}, \t y_max: {y_max}")


regr = LinearRegression()
regr.fit(X_train_scaled, y_train)

# grid in PCA space
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 300),
    np.linspace(y_min, y_max, 300)
)

grid = np.c_[xx.ravel(), yy.ravel()]

# project back to 13D
X_inv_pca = pca.inverse_transform(grid)


# continuous predictions
Z = regr.predict(X_inv_pca)

# convert continuous â†’ classes
threshold = 0.75     # example, you choose this
Z_class = np.where(Z < threshold, 0, Z)
Z_class = np.where((Z_class >= threshold) & (Z_class < 2*threshold), 1, Z_class)
Z_class = np.where(Z_class >= 2*threshold, 2, Z_class)

Z_class = Z_class.reshape(xx.shape)

# proper levels for filled regions
plt.contourf(xx, yy, Z_class, alpha=0.3, levels=[-0.5, 0.5, 1.5, 2.5])
plt.scatter(X_test_pca[:,0], X_test_pca[:,1], c=y_test, edgecolors='k')

plt.show()




